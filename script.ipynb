{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script used in case study of my Master's thesis utilizing the GitHub and Debricked API.\n",
    "Please read readme for more details and how to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install requests\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install pickle5\n",
    "import requests\n",
    "import base64\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = [{\n",
    "    'identifier': '',\n",
    "    'dependency_files': [],\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_api_token = ''\n",
    "base_repos_url = 'https://api.github.com/repos/'\n",
    "debricked_access_token = ''\n",
    "debricked_api_version = '1.0'\n",
    "debricked_api_url = f'https://debricked.com/api/{debricked_api_version}'\n",
    "\n",
    "headers = {}\n",
    "headers['Authorization'] = f\"token {github_api_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Created by Christian Normand\n",
    "    Copied from https://medium.com/@ctnormand1/the-four-quadrant-chart-3debe0dfdd77 \n",
    "    With custom modifications to improve readability\n",
    "\"\"\"\n",
    "def quadrant_chart(x, y, xtick_labels=None, ytick_labels=None, data_labels=None,\n",
    " highlight_quadrants=None, ax=None):\n",
    "    \"\"\"\n",
    "    Create the classic four-quadrant chart.\n",
    "    Args:\n",
    "        x -- array-like, the x-coordinates to plot\n",
    "        y -- array-like, the y-coordinates to plot\n",
    "        xtick_labels -- list, default: None, a two-value list xtick labels\n",
    "        ytick_labels -- list, default: None, a two-value list of ytick labels\n",
    "        data_labels -- array-like, default: None, data point annotations\n",
    "        highlight_quadrants -- list, default: None, list of quadrants to\n",
    "            emphasize (quadrants are numbered 1-4)\n",
    "        ax -- matplotlib.axes object, default: None, the user can pass their own\n",
    "            axes object if desired\n",
    "    \"\"\"\n",
    "    # allow user to specify their own axes\n",
    "    ax = ax if ax else plt.axes()\n",
    "\n",
    "    data = pd.DataFrame({'x': x, 'y': y, 'data_labels': data_labels})\n",
    "\n",
    "    # calculate averages up front to avoid repeated calculations\n",
    "    y_avg = data['y'].mean()\n",
    "    x_avg = data['x'].mean()\n",
    "\n",
    "    # set x limits\n",
    "    adj_x = max((data['x'].max() - x_avg), (x_avg - data['x'].min())) * 1.1\n",
    "    lb_x, ub_x = (x_avg - adj_x, x_avg + adj_x)\n",
    "    ax.set_xlim(lb_x, ub_x)\n",
    "\n",
    "    # set y limits\n",
    "    adj_y = max((data['y'].max() - y_avg), (y_avg - data['y'].min())) * 1.1\n",
    "    lb_y, ub_y = (y_avg - adj_y, y_avg + adj_y)\n",
    "    ax.set_ylim(lb_y, ub_y)\n",
    "\n",
    "    # set x tick labels\n",
    "    if xtick_labels:\n",
    "        ax.set_xticks([(x_avg - adj_x / 2), (x_avg + adj_x / 2)])\n",
    "        ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    # set y tick labels\n",
    "    if ytick_labels:\n",
    "        ax.set_yticks([(y_avg - adj_y / 2), (y_avg + adj_y / 2)])\n",
    "        ax.set_yticklabels(ytick_labels, rotation='vertical', va='center')\n",
    "\n",
    "    # determine which points to highlight\n",
    "    if highlight_quadrants:\n",
    "        quadrants = []\n",
    "        for x_val, y_val in zip(x, y):\n",
    "            q = []\n",
    "            if (x_val >= x_avg) and (y_val >= y_avg):\n",
    "                q.append(1)\n",
    "            if (x_val <= x_avg) and (y_val >= y_avg):\n",
    "                q.append(2)\n",
    "            if (x_val <= x_avg) and (y_val <= y_avg):\n",
    "                q.append(3)\n",
    "            if (x_val >= x_avg) and (y_val <= y_avg):\n",
    "                q.append(4)\n",
    "            quadrants.append(q)\n",
    "        data['quadrant'] = quadrants\n",
    "\n",
    "        # boolean mask - True = highlight, False = don't highlight\n",
    "        highlight = data['quadrant'].apply(lambda q: len(set(\n",
    "        highlight_quadrants) & set(q)) > 0)\n",
    "\n",
    "        # plot the non-highlighted points within the conditional block\n",
    "        ax.scatter(data['x'][~highlight], data['y'][~highlight], alpha=0.5,\n",
    "        c='lightblue', edgecolor='darkblue', zorder=99)\n",
    "        data = data[highlight]\n",
    "\n",
    "    # plot remaining points and quadrant lines\n",
    "    ax.scatter(x=data['x'], y=data['y'], c='lightblue', edgecolor='darkblue',\n",
    "    zorder=99)\n",
    "    ax.axvline(x_avg, c='k', lw=1)\n",
    "    ax.axhline(y_avg, c='k', lw=1)\n",
    "\n",
    "    # add data labels\n",
    "    for ix, row in data.iterrows():\n",
    "        labels = ax.annotate(row['data_labels'], (row['x'], row['y']), xytext=(2, [-38,18][random.randrange(2)]),\n",
    "        textcoords='offset pixels', fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_language_data(repos: list, overwrite_existing_data = False) -> list:\n",
    "    # prepare for data export\n",
    "    file_name_repo_language_data = \"all_repo_language_data.pkl\"\n",
    "    existing_file_found = False\n",
    "    try:\n",
    "        f = open(file_name_repo_language_data, \"rb\")\n",
    "        existing_file_found = True\n",
    "    except:\n",
    "        existing_file_found = False\n",
    "    if (existing_file_found and not overwrite_existing_data):\n",
    "        f = open(file_name_repo_language_data, \"rb\")\n",
    "        return pickle.load(f)\n",
    "    else:\n",
    "        all_languages = []\n",
    "        for repo in repos:\n",
    "            res = requests.get(f\"{base_repos_url}{repo['identifier']}/languages\" , headers=headers )\n",
    "            data = res.json()\n",
    "            sum_byte = sum(data.values())\n",
    "            lang_share = {k: round((v / sum_byte), 6 ) for k, v in data.items()}\n",
    "            all_languages += lang_share.items()\n",
    "        # dump data to file\n",
    "        open_file = open(file_name_repo_language_data, \"wb\")\n",
    "        pickle.dump(all_languages, open_file)\n",
    "        open_file.close()\n",
    "        return all_languages\n",
    "\n",
    "def get_language_apperance(language_data: list):\n",
    "    sum_lang_apperances = {k: sum(x[0] == k for x in language_data) for k, v in language_data}\n",
    "    return dict(sorted(sum_lang_apperances.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "def get_language_extent(language_data: list):\n",
    "    lang_extent = {k: round(sum(x[1] for x in language_data if x[0] == k)/ len(repos), 4) for k, v in language_data}\n",
    "    # prune languages with extent 0 due to rounding \n",
    "    pruned = {k: v for k, v in lang_extent.items() if v > 0}\n",
    "    return dict(sorted(pruned.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start \n",
    "all_languages_data = get_repo_language_data(repos=repos)\n",
    "# print(f'all languages: {sorted(all_languages, key=lambda d: d[1])}')\n",
    "sum_lang_apperances = get_language_apperance(language_data=all_languages_data)\n",
    "print(f'lang apperances: \\n {sum_lang_apperances}')\n",
    "lang_extent = get_language_extent(language_data=all_languages_data)\n",
    "# lang_extent_sorted = dict(sorted(lang_extent.items(), key=lambda item: item[1], reverse=True))\n",
    "print(f'lang extent: \\n {lang_extent}')\n",
    "\n",
    "# plot pie chart\n",
    "# prune a bit more\n",
    "langs_filtered = {k: v for k, v in lang_extent.items() if v >= 0.01}\n",
    "print(langs_filtered)\n",
    "if len(langs_filtered) != len(lang_extent):\n",
    "    sum_langs_filtered = sum(langs_filtered.values())\n",
    "    langs_filtered['Other'] = 1.0 - sum_langs_filtered\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = langs_filtered.keys()\n",
    "sizes = langs_filtered.values()\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(8, 8), dpi=200)\n",
    "ax1.pie(sizes, labels=labels, pctdistance=0.8,  autopct='%1.2f%%', startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "quadrant_chart(\n",
    "    x=sum_lang_apperances.values(),\n",
    "    y=lang_extent.values(),\n",
    "    xtick_labels=['Low', 'High'],\n",
    "    ytick_labels=['Low', 'High'],\n",
    "    data_labels=sum_lang_apperances.keys(),\n",
    "    highlight_quadrants=[1,2,4]\n",
    ")\n",
    "plt.title('Used languages in dPhoenix dependencies', fontsize=16)\n",
    "plt.ylabel('Extent of Usage', fontsize=14)\n",
    "plt.xlabel('Occurence of Language', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repos_pr_data(repos: list, overwrite_existing_data = False) -> list:\n",
    "\n",
    "    # prepare for data export\n",
    "    file_name_repo_pr_data = \"all_repo_pr_data.pkl\"\n",
    "    existing_file_found = False\n",
    "    try:\n",
    "        f = open(file_name_repo_pr_data, \"rb\")\n",
    "        existing_file_found = True\n",
    "    except:\n",
    "        existing_file_found = False\n",
    "    if (existing_file_found and not overwrite_existing_data):\n",
    "        f = open(file_name_repo_pr_data, \"rb\")\n",
    "        return pickle.load(f)\n",
    "    else:\n",
    "        repo_data = []\n",
    "        # set today for age calculations outside the for loop to not get distortions between the different repos\n",
    "        today = datetime.now()\n",
    "        for repo in repos:\n",
    "            # get last 1000 prs\n",
    "            all_prs = get_last_1000_pull_requests(repo=repo[\"identifier\"])\n",
    "\n",
    "            # get statistics for PRs in general\n",
    "            closed_prs = list(filter(lambda d: d.get('state') == 'closed', all_prs))\n",
    "            # only check closed for merged as open PRs can not be merged\n",
    "            merged_prs, not_merged_prs = filter_by_merge_status(pull_requests=closed_prs)\n",
    "            # share of closed PRs not being merged\n",
    "            share_of_not_merged_regular_prs = round(len(not_merged_prs) / len(closed_prs), 4) * 100\n",
    "            merge_durations_regular_prs = get_durations_for_merge(merged_prs)\n",
    "\n",
    "            dependabot_prs = list(filter(lambda d: d.get('user').get('login') == 'dependabot[bot]', all_prs))\n",
    "            if len(dependabot_prs) != 0:\n",
    "                open_dependabot_prs = list(filter(lambda d: d.get('state') == 'open', dependabot_prs))\n",
    "                age_deltas = pd.DataFrame([(today - datetime.strptime(open_pr.get('created_at'), \"%Y-%m-%dT%H:%M:%SZ\")) for open_pr in open_dependabot_prs])\n",
    "                closed_dependabot_prs = list(filter(lambda d: d.get('state') == 'closed', dependabot_prs))\n",
    "                # only check closed for merged as open PRs can not be merged\n",
    "                merged_dependabot_prs, not_merged_dependabot_prs = filter_by_merge_status(pull_requests=closed_dependabot_prs)\n",
    "                # share of closed PRs not being merged\n",
    "                share_of_not_merged_dependabot_prs = round(len(not_merged_dependabot_prs) / len(closed_dependabot_prs), 4) * 100\n",
    "\n",
    "                # calculate mean age of merged\n",
    "                if (len(merged_dependabot_prs) > 0):\n",
    "                    merge_durations_dependabot_prs = get_durations_for_merge(merged_dependabot_prs)\n",
    "\n",
    "                repo_data.append({\n",
    "                    'repo_name': repo['identifier'],\n",
    "                    'general': {\n",
    "                        'raw_data_prs': all_prs,\n",
    "                        'closed_pull_requests': closed_prs,\n",
    "                        'merged_pull_requests': merged_prs,\n",
    "                        'not_merged_pull_requests': not_merged_prs,\n",
    "                        'share_closed_not_merged_pull_requests': share_of_not_merged_regular_prs,\n",
    "                        'merge_durations': merge_durations_regular_prs if len(merged_prs) > 0 else None\n",
    "                    },\n",
    "                    'dependabot': {\n",
    "                        'raw_data_prs': dependabot_prs,\n",
    "                        'open_pull_requests': {\n",
    "                            'raw_data_prs': open_dependabot_prs,\n",
    "                            'age_deltas': age_deltas,\n",
    "                        },\n",
    "                        'closed_pull_requests': {\n",
    "                            'raw_data_prs': closed_dependabot_prs,\n",
    "                            'merged_pull_requests': merged_dependabot_prs,\n",
    "                            'not_merged_pull_requests': not_merged_dependabot_prs,\n",
    "                            'share_closed_not_merged_pull_requests': share_of_not_merged_dependabot_prs,\n",
    "                            'merge_durations': merge_durations_dependabot_prs if len(merged_dependabot_prs) > 0 else None\n",
    "                        }\n",
    "                    \n",
    "                    }\n",
    "                })\n",
    "            else:\n",
    "                repo_data.append({\n",
    "                    'repo_name': repo['identifier'],\n",
    "                    'general': {\n",
    "                        'raw_data_prs': all_prs,\n",
    "                        'closed_pull_requests': closed_prs,\n",
    "                        'merged_pull_requests': merged_prs,\n",
    "                        'not_merged_pull_requests': not_merged_prs,\n",
    "                        'share_closed_not_merged_pull_requests': share_of_not_merged_regular_prs,\n",
    "                        'merge_durations': merge_durations_regular_prs if len(merged_prs) > 0 else None\n",
    "                    },\n",
    "                    'dependabot': {\n",
    "                    }\n",
    "                })\n",
    "        # dump data to file\n",
    "        open_file = open(file_name_repo_pr_data, \"wb\")\n",
    "        pickle.dump(repo_data, open_file)\n",
    "        open_file.close()\n",
    "        return repo_data\n",
    "\n",
    "def print_repo_statistics(repo_data: list):\n",
    "    for r in repo_data:\n",
    "        # output styling\n",
    "        print('________________________________')\n",
    "        print(f'{r[\"repo_name\"]}:')\n",
    "        print(f'found PRs: {len(r[\"general\"][\"raw_data_prs\"])}')\n",
    "        print(f'closed PRs in general: {len(r[\"general\"][\"closed_pull_requests\"])}')\n",
    "        print(f'closed and merged PRs: {len(r[\"general\"][\"merged_pull_requests\"])}')\n",
    "        print(f'closed and not merged PRs: {len(r[\"general\"][\"not_merged_pull_requests\"])}')\n",
    "        print(f'share of closed but not merged: {r[\"general\"][\"share_closed_not_merged_pull_requests\"]}%')\n",
    "        print(f'time to merge of PR:')\n",
    "        if r[\"general\"][\"merge_durations\"]:\n",
    "            print_timedelta_statistics(r[\"general\"][\"merge_durations\"])\n",
    "        print('____________')\n",
    "        if len(r[\"dependabot\"]) != 0:\n",
    "            print(f'Dependabot PRs: {len(r[\"dependabot\"][\"raw_data_prs\"])}')\n",
    "            print(f'open PRs: {len(r[\"dependabot\"][\"open_pull_requests\"][\"raw_data_prs\"])}')\n",
    "            print('age of open dependabot PRs:')\n",
    "            print_timedelta_statistics(r[\"dependabot\"][\"open_pull_requests\"][\"age_deltas\"])\n",
    "            print(f'closed PRs: {len(r[\"dependabot\"][\"closed_pull_requests\"][\"raw_data_prs\"])}')\n",
    "            print(f'closed and merged PRs: {len(r[\"dependabot\"][\"closed_pull_requests\"][\"merged_pull_requests\"])}')\n",
    "            print(f'closed and not merged PRs: {len(r[\"dependabot\"][\"closed_pull_requests\"][\"not_merged_pull_requests\"])}')\n",
    "            print(f'share of closed but not merged: {r[\"dependabot\"][\"closed_pull_requests\"][\"share_closed_not_merged_pull_requests\"]}%')\n",
    "            if (len(r[\"dependabot\"][\"closed_pull_requests\"][\"merged_pull_requests\"]) > 0):\n",
    "                print(f'time to merge of dependabot PR:')\n",
    "                print_timedelta_statistics(r[\"dependabot\"][\"closed_pull_requests\"][\"merge_durations\"])\n",
    "        else: \n",
    "            print(f'no dependabot usage in history of last 1000 PRs')\n",
    "\n",
    "\n",
    "def get_durations_for_merge(pull_requests: list):\n",
    "    merge_deltas = [(datetime.strptime(pr.get('merged_at'), \"%Y-%m-%dT%H:%M:%SZ\") - datetime.strptime(pr.get('created_at'), \"%Y-%m-%dT%H:%M:%SZ\")) for pr in pull_requests]\n",
    "    return merge_deltas\n",
    "    \n",
    "def print_timedelta_statistics(time_deltas: list):\n",
    "    time_deltas = pd.DataFrame(time_deltas)\n",
    "    print(f'mean: {np.mean(time_deltas)}')\n",
    "    print(f'std: {np.std(time_deltas)}')\n",
    "    print(f'min: {np.min(time_deltas)}')\n",
    "    print(f'max: {np.max(time_deltas)}')\n",
    "    \n",
    "def get_last_1000_pull_requests(repo: str):\n",
    "    pull_requests = []\n",
    "    response_status = 200\n",
    "    page_index = 1\n",
    "    while (response_status == 200 and page_index <= 10):\n",
    "        res = requests.get(f\"{base_repos_url}{repo}/pulls\" , headers=headers, params={\n",
    "            'per_page':100, 'page':page_index, 'state': 'all'\n",
    "        })\n",
    "        pull_requests += res.json()\n",
    "        response_status = res.status_code\n",
    "        page_index += 1\n",
    "    return pull_requests\n",
    "\n",
    "def get_last_1000_commits(repo: str):\n",
    "    commits = []\n",
    "    response_status = 200\n",
    "    page_index = 1\n",
    "    while (response_status == 200 and page_index <= 10):\n",
    "        res = requests.get(f\"{base_repos_url}{repo}/commits\" , headers=headers, params={\n",
    "            'per_page':100, 'page':page_index,\n",
    "        })\n",
    "        commits += res.json()\n",
    "        response_status = res.status_code\n",
    "        page_index += 1\n",
    "    return commits\n",
    "\n",
    "def filter_by_merge_status(pull_requests: list):\n",
    "    merged_dependabot_prs = []\n",
    "    not_merged_dependabot_prs = []\n",
    "    for pr in pull_requests:\n",
    "        if pr.get('merged_at') is not None:\n",
    "            merged_dependabot_prs.append(pr)\n",
    "        else:\n",
    "            not_merged_dependabot_prs.append(pr)\n",
    "    return merged_dependabot_prs, not_merged_dependabot_prs\n",
    "\n",
    "def plot_boxplot_merge_durations(time_deltas: list, dependency_names: list, fig_title: str, manual_limits: tuple):\n",
    "    deltas_in_days = [[ delta.total_seconds()/(60*60*24) for delta in deltas] for deltas in time_deltas]\n",
    "    fig = plt.figure(figsize =(2.5*len(dependency_names), 6), dpi=200)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Creating plot\n",
    "    plt.boxplot(deltas_in_days)\n",
    "    # x-axis labels\n",
    "    ax.set_xticklabels(f'{dependency_name.split(\"/\")[0]}/\\n{dependency_name.split(\"/\")[1]}\\n(n= {len(time_deltas[ind])})' for ind, dependency_name in enumerate(dependency_names) )\n",
    "    # Adding title\n",
    "    plt.title(fig_title)\n",
    "    if (manual_limits):\n",
    "        plt.ylim(manual_limits)\n",
    "    # show plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_violinplot_merge_durations(time_deltas: list, dependency_names: list, fig_title: str, manual_limits: tuple):\n",
    "    deltas_in_days = [[ delta.total_seconds()/(60*60*24) for delta in deltas] for deltas in time_deltas]\n",
    "    fig = plt.figure(figsize =(2.5*len(dependency_names), 6), dpi=200)\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Creating plot\n",
    "    plt.violinplot(deltas_in_days)\n",
    "    # sns.violinplot(deltas_in_days)\n",
    "    # x-axis labels\n",
    "    ax.set_xticklabels(f'{dependency_name.split(\"/\")[0]}/\\n{dependency_name.split(\"/\")[1]}\\n(n= {len(time_deltas[ind])})' for ind, dependency_name in enumerate(dependency_names) )\n",
    "    # Adding title\n",
    "    plt.title(fig_title)\n",
    "    if (manual_limits):\n",
    "        plt.ylim(manual_limits)\n",
    "    # show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n",
    "repo_data = get_repos_pr_data(repos=repos)\n",
    "print_repo_statistics(repo_data=repo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot general pr times\n",
    "plot_boxplot_merge_durations([repo['general']['merge_durations'] for repo in repo_data[:5] if repo['general']['merge_durations'] != None], [repo['repo_name'] for repo in repo_data[:5] if repo['general']['merge_durations'] != None], fig_title=\"Days unitl opened pull requests get merged\", manual_limits=(-5, 180))\n",
    "# plit up in two figs\n",
    "plot_boxplot_merge_durations([repo['general']['merge_durations'] for repo in repo_data[5:] if repo['general']['merge_durations'] != None], [repo['repo_name'] for repo in repo_data[5:] if repo['general']['merge_durations'] != None], fig_title=\"Days unitl opened pull requests get merged\", manual_limits=(-5, 180))\n",
    "# plot general pr times\n",
    "plot_violinplot_merge_durations([repo['general']['merge_durations'] for repo in repo_data[:5] if repo['general']['merge_durations'] != None], [repo['repo_name'] for repo in repo_data[:5] if repo['general']['merge_durations'] != None], fig_title=\"Days unitl opened pull requests get merged\", manual_limits=(-5, 180))\n",
    "# plit up in two figs\n",
    "plot_violinplot_merge_durations([repo['general']['merge_durations'] for repo in repo_data[5:] if repo['general']['merge_durations'] != None], [repo['repo_name'] for repo in repo_data[5:] if repo['general']['merge_durations'] != None], fig_title=\"Days unitl opened pull requests get merged\", manual_limits=(-5, 180))\n",
    "\n",
    "# plot dependabot times\n",
    "plot_boxplot_merge_durations([repo['dependabot']['closed_pull_requests']['merge_durations'] for repo in repo_data if repo['dependabot'] != {} and repo['dependabot']['closed_pull_requests']['merge_durations'] != None], [repo['repo_name'] for repo in repo_data if repo['dependabot'] != {}and repo['dependabot']['closed_pull_requests']['merge_durations'] != None], fig_title=\"Days until opened Dependabot pull requests get merged\", manual_limits=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_share_not_merged = [repo['general']['share_closed_not_merged_pull_requests'] for repo in repo_data[1:]]\n",
    "print(mean_share_not_merged)\n",
    "print(f'mean: {np.mean(mean_share_not_merged)}')\n",
    "print(f'std: {np.std(mean_share_not_merged)}')\n",
    "print(f'min: {np.min(mean_share_not_merged)}')\n",
    "print(f'max: {np.max(mean_share_not_merged)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_share_not_merged = [repo['dependabot']['closed_pull_requests']['share_closed_not_merged_pull_requests'] for repo in repo_data[1:] if repo['dependabot'] != {}]\n",
    "print(mean_share_not_merged)\n",
    "print(f'mean: {np.mean(mean_share_not_merged)}')\n",
    "print(f'std: {np.std(mean_share_not_merged)}')\n",
    "print(f'min: {np.min(mean_share_not_merged)}')\n",
    "print(f'max: {np.max(mean_share_not_merged)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_avrg_merge_time = [np.mean(repo['general']['merge_durations']) for repo in repo_data[1:]]\n",
    "print(f'mean: {np.mean(mean_avrg_merge_time)}')\n",
    "# print(f'std: {np.std(mean_avrg_merge_time)}')\n",
    "print(f'min: {np.min(mean_avrg_merge_time)}')\n",
    "print(f'max: {np.max(mean_avrg_merge_time)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_debricked_access_token():\n",
    "    # refresh access token\n",
    "    refresh_url = f'https://debricked.com/api/login_refresh?refresh_token={debricked_access_token}'\n",
    "    res = requests.post(refresh_url)\n",
    "    data = res.json()\n",
    "    headers_debricked = {}\n",
    "    headers_debricked['Authorization'] = f\"Bearer {data['token']}\"\n",
    "    return headers_debricked\n",
    "    \n",
    "def get_repo_dependencies(repo_id):\n",
    "     dependencies = []\n",
    "     page_index = 1\n",
    "     data_page = None\n",
    "     while (data_page != []):\n",
    "          res = requests.get(debricked_api_url + '/open/dependencies/get-dependencies',\n",
    "          headers=headers_debricked, params={\n",
    "               'repositoryId': repo_id,\n",
    "               'sortColumn': 'name',\n",
    "               'order': 'desc',\n",
    "               'rowsPerPage': 50,\n",
    "               'page': page_index\n",
    "          })\n",
    "          data = res.json()\n",
    "          dependencies += data['dependencies']\n",
    "          data_page = data['dependencies']\n",
    "          page_index += 1\n",
    "     print(dependencies)\n",
    "     return dependencies\n",
    "\n",
    "def get_repo_vulnerabilities(repo_id):\n",
    "   vulnerabilities = []\n",
    "   page_index = 1\n",
    "   data_page = None\n",
    "   while (data_page != []):\n",
    "      res = requests.get(debricked_api_url + '/open/vulnerabilities/get-vulnerabilities',\n",
    "      headers=headers_debricked, params={\n",
    "         'repositoryId': repo_id,\n",
    "         'sortColumn': 'name',\n",
    "         'order': 'desc',\n",
    "         'rowsPerPage': 50,\n",
    "         'page': page_index\n",
    "      })\n",
    "      data = res.json()\n",
    "      vulnerabilities += data['vulnerabilities']\n",
    "      data_page = data['vulnerabilities']\n",
    "      page_index += 1\n",
    "   return vulnerabilities\n",
    "\n",
    "def get_vulnerabilities_with_fix(vulnerabilities: list):\n",
    "     vulns_with_fix = list(filter(lambda vuln: vuln['fixesAndExploits'][0]['type'] == 'manual-fix', vulnerabilities))\n",
    "     return vulns_with_fix\n",
    "\n",
    "def plot_as_bar_chart(repo_data: list):\n",
    "\n",
    "     labels = [f'{repo[\"repo_name\"].split(\"/\")[0]}/\\n{repo[\"repo_name\"].split(\"/\")[1]}' for repo in repo_data]\n",
    "     dependencies  = [repo['nmb_dependencies'] for repo in repo_data]\n",
    "     vulnerabilities = [repo['nmb_vulnerabilities'] for repo in repo_data]\n",
    "     vulnerabilities_with_fix = [repo['nmb_vulnerabilities_with_fix'] for repo in repo_data]\n",
    "\n",
    "     x = np.arange(len(labels))  # the label locations\n",
    "     width = 0.3  # the width of the bars\n",
    "\n",
    "     fig = plt.figure(figsize =(2*len(labels), 6), dpi=200)\n",
    "     ax = fig.add_subplot(111)\n",
    "     rects1 = ax.bar(x - width, dependencies, width, label='Dependencies')\n",
    "     rects2 = ax.bar(x , vulnerabilities, width, label='Vulnerabilities')\n",
    "     rects3 = ax.bar(x + width, vulnerabilities_with_fix, width, label='Vulnerabilities having a fix')\n",
    "\n",
    "     # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "     ax.set_ylabel('Quantity')\n",
    "     ax.set_title('Dependencies and vulnerabilties by repository')\n",
    "     ax.set_xticks(x, labels)\n",
    "     ax.legend()\n",
    "\n",
    "     ax.bar_label(rects1, padding=3)\n",
    "     ax.bar_label(rects2, padding=3)\n",
    "     ax.bar_label(rects3, padding=3)\n",
    "\n",
    "     fig.tight_layout()\n",
    "\n",
    "     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload data to Debricked, you only need to run this command once!\n",
    "headers_debricked = refresh_debricked_access_token()\n",
    "for repo in repos:\n",
    "    if repo['dependency_files'] != []:\n",
    "        for dependency_file in repo['dependency_files']:\n",
    "            upload_id = ''\n",
    "            print(repo)\n",
    "            print(dependency_file)\n",
    "            res = requests.get(base_repos_url + f'{repo[\"identifier\"]}/contents/{dependency_file}', headers=headers )\n",
    "            data = res.json()\n",
    "            res = requests.get(data['download_url'])\n",
    "            file_content = res.content.decode('utf-8')\n",
    "            if upload_id is not '':\n",
    "                data = {\n",
    "                    'repositoryName': repo[\"identifier\"],\n",
    "                    'commitName':'analysis',\n",
    "                    'upload_id': upload_id\n",
    "                } \n",
    "            else :\n",
    "                data = {\n",
    "                    'repositoryName': repo[\"identifier\"],\n",
    "                    'commitName':'analysis'\n",
    "                } \n",
    "            url = 'https://debricked.com/api/1.0'\n",
    "            res = requests.post(debricked_api_url + '/open/uploads/dependencies/files',\n",
    "                files = {\"fileData\": file_content}, headers=headers_debricked, data=data )\n",
    "            data = res.json()\n",
    "            if upload_id is '':\n",
    "                upload_id = data['ciUploadId']\n",
    "\n",
    "        data = {\n",
    "            'ciUploadId': upload_id\n",
    "        }\n",
    "        res = requests.post(debricked_api_url + '/open/finishes/dependencies/files/uploads',\n",
    "            headers=headers_debricked, data=data )\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_debricked = refresh_debricked_access_token()\n",
    "res = requests.get(debricked_api_url + '/open/repositories/get-repositories',\n",
    "     headers=headers_debricked )\n",
    "all_user_repos = res.json()['repositories']\n",
    "repo_data_for_plotting = []\n",
    "for repo in repos:\n",
    "     if repo['dependency_files'] != []:\n",
    "          print('_________________________')\n",
    "          current_repo = list(filter(lambda r: r['name']['name'] == repo['identifier'], all_user_repos))\n",
    "          current_repo_id = current_repo[0]['id']\n",
    "          print(f'{current_repo[0][\"name\"][\"name\"]}:')\n",
    "          nmb_dependencies = len(get_repo_dependencies(current_repo_id))\n",
    "          print(f'dependencies: {nmb_dependencies}')\n",
    "          vulnerabilities = get_repo_vulnerabilities(current_repo_id)\n",
    "          nmb_vulnerabilities = len(vulnerabilities)\n",
    "          nmb_vulnerabilities_with_fix = len(get_vulnerabilities_with_fix(vulnerabilities=vulnerabilities))\n",
    "          print(f'vulnerabilities: {nmb_vulnerabilities}')\n",
    "          print(f'from those having a fix: {nmb_vulnerabilities_with_fix}')\n",
    "\n",
    "          repo_data_for_plotting.append({\n",
    "               'repo_name': repo['identifier'],\n",
    "               'nmb_dependencies': nmb_dependencies,\n",
    "               'nmb_vulnerabilities': nmb_vulnerabilities,\n",
    "               'nmb_vulnerabilities_with_fix': nmb_vulnerabilities_with_fix\n",
    "          })\n",
    "     else:\n",
    "          print('_________________________')\n",
    "          print(f'{repo[\"identifier\"]}:')\n",
    "          print(f'no dependency files added!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_as_bar_chart(repo_data=repo_data_for_plotting[1:])\n",
    "sum_dependencies = sum(repo['nmb_dependencies'] for repo in repo_data_for_plotting[1:])\n",
    "print(sum_dependencies)\n",
    "shares_dependencies_vulnerabilities= [repo['nmb_vulnerabilities']/repo['nmb_dependencies'] for repo in repo_data_for_plotting[1:]]\n",
    "print(f'shares_dependencies_vulnerabilities:')\n",
    "print(f'mean: {round(np.mean(shares_dependencies_vulnerabilities)*100,2)}%')\n",
    "print(f'std: {round(np.std(shares_dependencies_vulnerabilities)*100,2)}%')\n",
    "print(f'min: {round(np.min(shares_dependencies_vulnerabilities)*100,2)}%')\n",
    "print(f'max: {round(np.max(shares_dependencies_vulnerabilities)*100,2)}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
